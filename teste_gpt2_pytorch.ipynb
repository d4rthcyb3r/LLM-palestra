{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX9Q15__2QZ9",
        "outputId": "7309189d-1c75-4759-b193-30c9c02318d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Andrei-Aksionov/nanoGPTplus.git\n",
        "%cd nanoGPTplus/\n",
        "!ls\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMYNAiDT2Slr",
        "outputId": "4b584e69-94b9-4834-ff07-7aa56953e64d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPTplus'...\n",
            "remote: Enumerating objects: 347, done.\u001b[K\n",
            "remote: Counting objects: 100% (229/229), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 347 (delta 130), reused 118 (delta 75), pack-reused 118\u001b[K\n",
            "Receiving objects: 100% (347/347), 695.34 KiB | 17.83 MiB/s, done.\n",
            "Resolving deltas: 100% (164/164), done.\n",
            "/content/nanoGPTplus\n",
            "LICENSE    poetry.lock\t   README.md   src\n",
            "notebooks  pyproject.toml  references  tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uoew-hw42tXz",
        "outputId": "9014b8bd-a71b-44d3-add5-14929b67f71f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --quiet virtualenv\n",
        "!virtualenv venv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYPhp_i22ueb",
        "outputId": "e21395fb-d9eb-402e-e0c8-6016e989cf74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.2/3.3 MB\u001b[0m \u001b[31m161.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/468.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.5/468.5 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hcreated virtual environment CPython3.10.11.final.0-64 in 700ms\n",
            "  creator CPython3Posix(dest=/content/nanoGPTplus/venv, clear=False, no_vcs_ignore=False, global=False)\n",
            "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n",
            "    added seed packages: pip==23.1.2, setuptools==67.7.2, wheel==0.40.0\n",
            "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# there are some difficulties with standard exporting of environment variable,\n",
        "# so I use python's builtin `os` module\n",
        "os.environ[\"PATH\"] = f\"{os.getcwd()}/venv/bin:{os.environ['PATH']}\"\n",
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRVCxL4F2xb7",
        "outputId": "e686a16e-5944-4c6b-87e5-450ade4879b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPTplus/venv/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list\n",
        "\n",
        "%pip install --quiet -e .\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IgAQN4Y21JU",
        "outputId": "1a13f16c-c431-419e-875a-dab30733d43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package    Version\n",
            "---------- -------\n",
            "pip        23.1.2\n",
            "setuptools 67.7.2\n",
            "wheel      0.40.0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m135.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.6/797.6 kB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.8/426.8 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m118.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m124.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.2/178.2 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.3/242.3 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.6/769.6 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.0/154.0 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for nanogptplus (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/data/scripts/download_tiny_shakespeare.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8ASTZ_D3A0L",
        "outputId": "a00d0d74-ccbc-45cc-9bbf-977b474ca782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-05-02 19:09:11.513\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.data.downloader\u001b[0m:\u001b[36mdownload\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mDownloading https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt into /content/nanoGPTplus/data/raw/tiny_shakespeare\u001b[0m\n",
            "\u001b[32m2023-05-02 19:09:11.742\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.data.downloader\u001b[0m:\u001b[36mdownload\u001b[0m:\u001b[36m44\u001b[0m - \u001b[34m\u001b[1mDownloading is finished\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/model/train.py bigram --size large\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ3enPcT3CLa",
        "outputId": "e518839a-9d05-4eb0-d73a-abd995232ac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-05-02 19:09:57.234\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[34m\u001b[1mRandom seed is fixed for training.\u001b[0m\n",
            "\u001b[32m2023-05-02 19:09:57.234\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mLoading the data...\u001b[0m\n",
            "\u001b[32m2023-05-02 19:09:57.240\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mData is loaded.\u001b[0m\n",
            "\u001b[32m2023-05-02 19:09:57.241\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mStarting tokenizing...\u001b[0m\n",
            "\u001b[32m2023-05-02 19:09:57.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mTokenizing is done.\u001b[0m\n",
            "\u001b[32m2023-05-02 19:09:57.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mSaving tokenizer...\u001b[0m\n",
            "\u001b[32m2023-05-02 19:09:57.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mTokenizer is saved.\u001b[0m\n",
            "\u001b[32m2023-05-02 19:09:57.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mPreparing data loaders...\u001b[0m\n",
            "\u001b[32m2023-05-02 19:09:57.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mData loaders are prepared.\u001b[0m\n",
            "\u001b[32m2023-05-02 19:09:57.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mStaring training...\u001b[0m\n",
            "\u001b[32m2023-05-02 19:09:57.373\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m131\u001b[0m - \u001b[34m\u001b[1mLR warmup iters: 0\u001b[0m\n",
            "\u001b[32m2023-05-02 19:09:57.373\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m137\u001b[0m - \u001b[34m\u001b[1mLR decay iters: 31371\u001b[0m\n",
            "\u001b[32m2023-05-02 19:09:57.374\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m122\u001b[0m - \u001b[34m\u001b[1mTraining on 'cpu' device\u001b[0m\n",
            "=============== Epoch: 0 ===============\n",
            "train: 100% 31371/31371 [01:23<00:00, 376.24it/s, loss=2.47]\n",
            "eval: 100% 3486/3486 [00:06<00:00, 500.62it/s, loss=2.5]\n",
            "Eval averaged loss: 2.5011\n",
            "\u001b[32m2023-05-02 19:11:27.723\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m151\u001b[0m - \u001b[1mCurrent eval loss is `2.5011` which is smaller than current best loss of `inf`; saving the model...\u001b[0m\n",
            "\u001b[32m2023-05-02 19:11:27.724\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.model.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m157\u001b[0m - \u001b[1mBest model is saved.\u001b[0m\n",
            "\u001b[32m2023-05-02 19:11:27.725\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m156\u001b[0m - \u001b[1mTraining is finished\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/model/generate.py bigram --size large --max-new-tokens 100 --fix-seed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbwsMGGJ3GBf",
        "outputId": "123281ba-a79e-481f-8186-441d757f1989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-05-02 19:21:21.424\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mRandom seed is fixed for token generation.\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:21.436\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m104\u001b[0m - \u001b[34m\u001b[1mGenerating tokens on 'cpu' device\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:21.466\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m120\u001b[0m - \u001b[1mNew generated tokens:  adat I ican wiayoul, devend w ou, brhthishit r bet y hal a ho ffe hochour o he isollyowe gh irt whot\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:21.466\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m121\u001b[0m - \u001b[34m\u001b[1mToken generation took: 0.0306 seconds\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"gpt_size\"] = \"medium\"\n",
        "\n"
      ],
      "metadata": {
        "id": "rn2SwbnL3MY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/model/train.py gpt --size $gpt_size --dataset-fraction 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wujWyeVL3Pyq",
        "outputId": "4eccbb78-e788-4555-e0ee-feff92c4a1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-05-02 19:21:32.413\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m60\u001b[0m - \u001b[34m\u001b[1mRandom seed is fixed for training.\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:32.414\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mLoading the data...\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:32.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mData is loaded.\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:32.421\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mStarting tokenizing...\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:32.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m76\u001b[0m - \u001b[1mTokenizing is done.\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:32.551\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m79\u001b[0m - \u001b[1mSaving tokenizer...\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:32.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mTokenizer is saved.\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:32.554\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mPreparing data loaders...\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:32.555\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mData loaders are prepared.\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:32.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m106\u001b[0m - \u001b[1mStaring training...\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:32.703\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m114\u001b[0m - \u001b[34m\u001b[1mGPT language model is created with number of parameters: 10.65 million\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:32.703\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m131\u001b[0m - \u001b[34m\u001b[1mLR warmup iters: 156\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:32.703\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m137\u001b[0m - \u001b[34m\u001b[1mLR decay iters: 1490\u001b[0m\n",
            "\u001b[32m2023-05-02 19:21:32.705\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.trainer\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m122\u001b[0m - \u001b[34m\u001b[1mTraining on 'cpu' device\u001b[0m\n",
            "=============== Epoch: 0 ===============\n",
            "train:  21% 324/1569 [5:36:33<21:31:00, 62.22s/it, loss=2.42]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/model/generate.py gpt --size $gpt_size --max-new-tokens 1000 --fix-seed\n"
      ],
      "metadata": {
        "id": "xD4JUZ7k3UBW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1d206cd-ab03-4877-fffd-fcda2f54bdb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-05-02 19:19:25.522\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mgenerate_new_tokens\u001b[0m:\u001b[36m69\u001b[0m - \u001b[34m\u001b[1mRandom seed is fixed for token generation.\u001b[0m\n",
            "\u001b[32m2023-05-02 19:19:25.670\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36msrc.model.gpt_language_model.gpt\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m114\u001b[0m - \u001b[34m\u001b[1mGPT language model is created with number of parameters: 10.65 million\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/nanoGPTplus/src/model/generate.py\", line 222, in <module>\n",
            "    main()\n",
            "  File \"/content/nanoGPTplus/src/model/generate.py\", line 218, in main\n",
            "    generate_new_tokens(model_name, **args)\n",
            "  File \"/content/nanoGPTplus/src/model/generate.py\", line 95, in generate_new_tokens\n",
            "    load_checkpoint(model_config.checkpoint_model_path, model)\n",
            "  File \"/content/nanoGPTplus/src/utils/model.py\", line 69, in load_checkpoint\n",
            "    model.load_state_dict(torch.load(path))\n",
            "  File \"/content/nanoGPTplus/venv/lib/python3.10/site-packages/torch/serialization.py\", line 771, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/content/nanoGPTplus/venv/lib/python3.10/site-packages/torch/serialization.py\", line 270, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/content/nanoGPTplus/venv/lib/python3.10/site-packages/torch/serialization.py\", line 251, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/nanoGPTplus/models/gpt_model_medium.pth.tar'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"gpt2_config\"] = \"gpt2-medium\"\n",
        "os.environ[\"max_new_tokens\"] = \"1000\"\n",
        "os.environ[\"continue_tokens\"] = \"My name is Giovanni Giorgio but everybody calls me \""
      ],
      "metadata": {
        "id": "WIGW80UE3Zym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/model/generate.py gpt --gpt2-config $gpt2_config --max-new-tokens \"$((max_new_tokens))\" --fix-seed --continue-tokens \"$continue_tokens\"\n"
      ],
      "metadata": {
        "id": "OEHxl62N3b5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/model/generate.py gpt --gpt2-config $gpt2_config --max-new-tokens \"$((max_new_tokens))\" --fix-seed --continue-tokens \"$continue_tokens\" --use-kv-cache\n"
      ],
      "metadata": {
        "id": "_WYVOMy03fSv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}